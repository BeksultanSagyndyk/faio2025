https://contest.yandex.ru/contest/69765/problems/F/


## Введение

Часто в алгоритмах машинного обучения возникает необходимость классифицировать данные. В данной задаче необходимо реализовать \textbf{метод опорных векторов} (support vector machine — SVM), который относится к семейству линейных классификаторов.

Для простоты рассмотрим $n$ точек на двумерной плоскости. Каждая из этих точек принадлежит одному из двух классов, обозначенных как $l_i=\pm{1}$. Гарантируется, что точки можно разделить прямой так, что по одну сторону от нее будут находиться точки одного класса. Искомых прямых может быть много, поэтому будем искать такую прямую $L$, что сумма расстояний от нее до двух ближайших точек, лежащих по разные стороны, будет максимальна. Это способствует более уверенной классификации.

<img width="874" height="684" alt="image" src="https://github.com/user-attachments/assets/f670e35b-8e70-4687-bf25-b532eff08693" />


## Алгоритм классификации

Пусть линия $L$ задается уравнением $ax + by + c = 0$. Тогда линии $L'$ и $L''$, параллельные $L$, можно описать уравнениями $ax + by + c + \delta = 0$ и $ax + by + c - \delta = 0$ соответственно, где $\delta>0$. Для удобства прямые $L′$ и $L''$, проходящие через эти две точки, будем называть опорными векторами. Расстояние между прямыми  $L'$ и $L''$:
  
\begin{equation}
    H = \frac{2\delta}{\left|\vec{n}\right|}
\end{equation}

Точку $(x, y)$ будем классифицировать следующим образом:

\[
a(x, y)  =
\begin{cases}
+1  & \text{если $ax+by+c \geq 0$,} \\
-1 & \text{иначе} 
\end{cases}
\]

<img width="1288" height="1004" alt="image" src="https://github.com/user-attachments/assets/7ea8f647-6d3f-4684-8843-f0e369cff0ef" />


## Функция потерь

Алгоритм будем штрафовать при неправильной классификации:

\[
f(x, y, l)  =
\begin{cases}
0  & \text{если $a(x, y) = l$,} \\
\left|\vec{n}\right| d(L, x, y) & \text{иначе} 
\end{cases}
\]

Здесь $d(L,x,y)$ обозначает расстояние от точки с координатами $(x,y)$ до прямой $L$.

Также мы ходим, чтобы зазор между опорными векторами $L'$ и $L''$ был как можно большим, поэтому рассмотрим функцию потерь (гиперпараметр $\lambda$ алгоритма нужно подобрать самостоятельно): 

\begin{equation}
    Loss = \lambda {\left|\vec{n}\right|} ^2 + \frac{1}{k} \sum_{i=1}^{k}f(x_i, y_i, l_i)
\end{equation}

Ваша задача найти параметры $a, b, c$ прямой $L$, используя градиентный спуск который будет минимизировать $Loss$.

## Метрика

В контесте будем измерять для каждого теста долю правильных ответов (точность), и итоговая метрика определяется по этой формуле:

\[
score  =
\begin{cases}
0  & \text{если точность < 0.97} \\
100 \cdot \text{точность} & \text{иначе} 
\end{cases}
\]

Итоговый балл будет средним баллом по всем тестовым наборам.

## Формат ввода

```
k

x_1 y_1 l_1


x_2 y_2 l_2

...

x_k y_k l_k
```

### Описание

$k$ - число точек.

$x_i, y_i$ - координаты i-ый точки

$l_i$ - класс i-ый точки

### Ограничения

$1000 < k < 4000$


$-100000 <  x_i, y_i < 100000$

### Пример ввода

```
10

2 6 -1

4 -8 1

9 -1 -1

5 2 -1

-9 7 -1

10 4 -1

2 -8 1

-6 -4 1

-10 -4 1

-2 8 -1
```

## Формат вывода
$a$ $b$ $c$

### Пример вывода

\vspace{5mm}
```
1 1 0
```

